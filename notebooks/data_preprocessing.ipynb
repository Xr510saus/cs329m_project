{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import all_paths_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DIR_NAME = 'split_cpps'\n",
    "LABEL_DIR_NAME = 'split_labels'\n",
    "\n",
    "file_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "proj_dir = os.path.dirname(file_dir)\n",
    "\n",
    "code_dir = f'{proj_dir}/{CODE_DIR_NAME}'\n",
    "label_dir = f'{proj_dir}/{LABEL_DIR_NAME}'\n",
    "\n",
    "if not all_paths_exist([code_dir, label_dir]):\n",
    "    err = f'Missing \"{CODE_DIR_NAME}\" & \"{LABEL_DIR_NAME}\" under \"{proj_dir}\"'\n",
    "    raise Exception(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['additional_functions_0', 'additional_functions_1', 'addrdb_0', 'addrdb_1', 'addrdb_10', 'addrdb_2', 'addrdb_3', 'addrdb_4', 'addrdb_5', 'addrdb_6', 'addrdb_7', 'addrdb_8', 'addrdb_9', 'addresstype_0', 'addresstype_1', 'addresstype_10', 'addresstype_11', 'addresstype_12', 'addresstype_13', 'addresstype_14', 'addresstype_15', 'addresstype_16', 'addresstype_17', 'addresstype_18', 'addresstype_19', 'addresstype_2', 'addresstype_20', 'addresstype_21', 'addresstype_22', 'addresstype_3', 'addresstype_4', 'addresstype_5', 'addresstype_6', 'addresstype_7', 'addresstype_8', 'addresstype_9', 'addrman_0', 'addrman_1', 'addrman_10', 'addrman_11', 'addrman_12', 'addrman_13', 'addrman_14', 'addrman_15', 'addrman_16', 'addrman_17', 'addrman_18', 'addrman_19', 'addrman_2', 'addrman_20', 'addrman_21', 'addrman_22', 'addrman_23', 'addrman_24', 'addrman_25', 'addrman_26', 'addrman_27', 'addrman_28', 'addrman_29', 'addrman_3', 'addrman_30', 'addrman_31', 'addrman_32', 'addrman_33', 'addrman_34', 'addrman_35', 'addrman_36', 'addrman_37', 'addrman_38', 'addrman_39', 'addrman_4', 'addrman_40', 'addrman_41', 'addrman_42', 'addrman_43', 'addrman_44', 'addrman_45', 'addrman_46', 'addrman_47', 'addrman_48', 'addrman_49', 'addrman_5', 'addrman_50', 'addrman_51', 'addrman_52', 'addrman_53', 'addrman_54', 'addrman_55', 'addrman_56', 'addrman_57', 'addrman_6', 'addrman_7', 'addrman_8', 'addrman_9', 'auto_updater_0', 'auto_updater_1', 'auto_updater_2', 'auto_updater_3', 'auto_updater_4', 'auto_updater_5', 'auto_updater_6', 'command_line_args_0', 'command_line_args_1', 'command_line_args_2', 'function_data_0', 'function_data_1', 'function_data_10', 'function_data_11', 'function_data_12', 'function_data_13', 'function_data_14', 'function_data_15', 'function_data_16', 'function_data_17', 'function_data_18', 'function_data_19', 'function_data_2', 'function_data_20', 'function_data_21', 'function_data_22', 'function_data_23', 'function_data_24', 'function_data_25', 'function_data_26', 'function_data_3', 'function_data_4', 'function_data_5', 'function_data_6', 'function_data_7', 'function_data_8', 'function_data_9', 'gpt_functions_0', 'gpt_functions_1', 'gpt_functions_10', 'gpt_functions_11', 'gpt_functions_12', 'gpt_functions_13', 'gpt_functions_14', 'gpt_functions_15', 'gpt_functions_16', 'gpt_functions_17', 'gpt_functions_18', 'gpt_functions_19', 'gpt_functions_2', 'gpt_functions_20', 'gpt_functions_21', 'gpt_functions_3', 'gpt_functions_4', 'gpt_functions_5', 'gpt_functions_6', 'gpt_functions_7', 'gpt_functions_8', 'gpt_functions_9', 'gpt_sef_0', 'gpt_sef_1', 'gpt_sef_10', 'gpt_sef_11', 'gpt_sef_12', 'gpt_sef_13', 'gpt_sef_14', 'gpt_sef_15', 'gpt_sef_2', 'gpt_sef_3', 'gpt_sef_4', 'gpt_sef_5', 'gpt_sef_6', 'gpt_sef_7', 'gpt_sef_8', 'gpt_sef_9', 'node_main_0', 'node_main_1', 'node_main_2', 'node_main_3', 'node_main_4', 'node_main_5', 'uv_stdio_fix_0']\n"
     ]
    }
   ],
   "source": [
    "file_names = []\n",
    "extension_tracker = dict()\n",
    "\n",
    "for _, _, files in os.walk(code_dir):\n",
    "    for file in files:\n",
    "        try:\n",
    "            if file[-3:] == '.cc':\n",
    "                name = file[:-3]\n",
    "                file_names.append(name)\n",
    "                extension_tracker[name] = 'cc'\n",
    "            elif file[-4:] == '.cpp':\n",
    "                name = file[:-4]\n",
    "                file_names.append(name)\n",
    "                extension_tracker[name] = 'cpp'\n",
    "        except:\n",
    "            # Exceptions likely occur due to the filename being less than 3/4 \n",
    "            # chars long, so we can skip since they cannot be the code files\n",
    "            # we're looking for.\n",
    "            continue\n",
    "        \n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataset_dir(new_dir: str, base_path: str = proj_dir)->str:\n",
    "    dir_path = f'{base_path}/datasets/{new_dir}'\n",
    "    \n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "    return dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0.10\n",
    "SEED = 42\n",
    "\n",
    "def create_train_test_split(label_filter: str, \n",
    "                            line_list: list[str], \n",
    "                            code_list: list[str], \n",
    "                            label_list: list[str])->tuple:\n",
    "    \n",
    "    filtered_list_idx = label_list == label_filter\n",
    "    \n",
    "    filtered_line_list = line_list[filtered_list_idx, np.newaxis]\n",
    "    filtered_code_list = code_list[filtered_list_idx, np.newaxis]\n",
    "    filtered_label_list = label_list[filtered_list_idx, np.newaxis]\n",
    "    filtered_combined_list = np.concatenate((filtered_line_list, \n",
    "                                             filtered_code_list,\n",
    "                                             filtered_label_list), axis=1)\n",
    "    \n",
    "    (combined_traineval, \n",
    "     combined_test, \n",
    "     label_traineval, _) = train_test_split(filtered_combined_list, \n",
    "                                            filtered_label_list, \n",
    "                                            test_size=TEST_SIZE, \n",
    "                                            random_state=SEED)\n",
    "    \n",
    "    (combined_train, \n",
    "     combined_eval, _, _) = train_test_split(combined_traineval, \n",
    "                                             label_traineval, \n",
    "                                             test_size=TEST_SIZE, \n",
    "                                             random_state=SEED)\n",
    "    \n",
    "    return combined_train, combined_eval, combined_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recombine_se_sef(se: np.ndarray, sef: np.ndarray)->np.ndarray:\n",
    "    return np.concatenate((se, sef), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tree_sitter_cpp as tscpp\n",
    "\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "LINE_DATASET = 'line_dataset'\n",
    "CUTOFF_DATASET = 'cutoff_dataset'\n",
    "AST_DATASET = 'ast_dataset'\n",
    "AST_NO_CODE_DATASET = 'ast_no_code_dataset'\n",
    "TEST_DATASET = 'test_dataset'\n",
    "COL_NAMES = ['line', 'code', 'label']\n",
    "\n",
    "parser = Parser(Language(tscpp.language()))\n",
    "\n",
    "def create_data_files(dataset_name: str, input_type: str = None)->None:\n",
    "    dataset_dir = set_dataset_dir(dataset_name)\n",
    "    if input_type == None:\n",
    "        print('in')\n",
    "        input_type = dataset_name\n",
    "    container = []\n",
    "\n",
    "    for name in file_names:\n",
    "        cpp_file = f'{code_dir}/{name}.{extension_tracker[name]}'\n",
    "        txt_file = f'{label_dir}/{name}.txt'\n",
    "        \n",
    "        if not all_paths_exist([cpp_file, txt_file]):\n",
    "            raise Exception(f'Could not find {cpp_file} and {txt_file}')\n",
    "        \n",
    "        with (open(cpp_file, 'r') as code, \n",
    "            open(txt_file, 'r') as labels):\n",
    "            code_lines = code.read().splitlines()\n",
    "            label_lines = labels.readlines()\n",
    "            \n",
    "            if len(code_lines) != len(label_lines):\n",
    "                raise Exception(f'Length mismatch for {name}.')\n",
    "            \n",
    "            for i in range(len(code_lines)):\n",
    "                curr_label = int(label_lines[i])\n",
    "                \n",
    "                if dataset_name == LINE_DATASET:\n",
    "                    curr_line = f'Line {i+1}: {code_lines[i].strip()}'\n",
    "                    stripped_code = [line.strip() for line in code_lines]\n",
    "                    curr_code_block = '\\t'.join(stripped_code)\n",
    "                    \n",
    "                elif dataset_name == CUTOFF_DATASET:\n",
    "                    curr_line = 'Side effect free?'\n",
    "                    stripped_code = [line.strip() for line in code_lines[:i+1]]\n",
    "                    curr_code_block = '\\t'.join(stripped_code)\n",
    "                    \n",
    "                elif dataset_name == AST_DATASET:\n",
    "                    stripped_code = [line.strip() for line in code_lines[:i+1]]\n",
    "                    tree = parser.parse(bytes('\\n'.join(stripped_code), \n",
    "                                              encoding='utf-8'))\n",
    "                    curr_line = str(tree.root_node)\n",
    "                    curr_code_block = '\\t'.join(stripped_code)\n",
    "                    \n",
    "                elif dataset_name == AST_NO_CODE_DATASET:\n",
    "                    stripped_code = [line.strip() for line in code_lines[:i+1]]\n",
    "                    tree = parser.parse(bytes('\\n'.join(stripped_code), \n",
    "                                              encoding='utf-8'))\n",
    "                    curr_line = str(tree.root_node)\n",
    "                    curr_code_block = 'na'\n",
    "                    \n",
    "                else:\n",
    "                    raise Exception(f'Dataset name \"{dataset_name}\" unknown.')\n",
    "                \n",
    "                write_str = [curr_line, curr_code_block, curr_label]\n",
    "                container.append(write_str)\n",
    "      \n",
    "    container = np.array(container)\n",
    "    line_list = container[:, 0]\n",
    "    code_list = container[:, 1]\n",
    "    label_list = container[:, 2]\n",
    "    \n",
    "    # Partitioning the dataset before splitting is necessary\n",
    "    # To ensure that the train, eval, and test splits\n",
    "    # All contain side effect functions\n",
    "    (se_combined_train, \n",
    "     se_combined_eval, \n",
    "     se_combined_test) = create_train_test_split('1', \n",
    "                                                 line_list, \n",
    "                                                 code_list, \n",
    "                                                 label_list)\n",
    "    \n",
    "    (sef_combined_train, \n",
    "     sef_combined_eval, \n",
    "     sef_combined_test) = create_train_test_split('0', \n",
    "                                                  line_list, \n",
    "                                                  code_list, \n",
    "                                                  label_list)\n",
    "    \n",
    "    # Recombine post-split\n",
    "    combined_train = recombine_se_sef(se_combined_train, sef_combined_train)\n",
    "    combined_eval = recombine_se_sef(se_combined_eval, sef_combined_eval)\n",
    "    combined_test = recombine_se_sef(se_combined_test, sef_combined_test)\n",
    "    \n",
    "    # Save to respective files\n",
    "    train_df = pd.DataFrame(combined_train, columns=COL_NAMES)\n",
    "    train_df.to_csv(f'{dataset_dir}/train.csv', index=False)\n",
    "    \n",
    "    eval_df = pd.DataFrame(combined_eval, columns=COL_NAMES)\n",
    "    eval_df.to_csv(f'{dataset_dir}/eval.csv', index=False)\n",
    "    \n",
    "    test_df = pd.DataFrame(combined_test, columns=COL_NAMES)\n",
    "    test_df.to_csv(f'{dataset_dir}/test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "create_data_files(LINE_DATASET)\n",
    "line_time = time.time()\n",
    "print(f'Time to finish line_dataset: {line_time - start_time} s')\n",
    "\n",
    "create_data_files(CUTOFF_DATASET)\n",
    "cutoff_time = time.time()\n",
    "print(f'Time to finish cutoff_dataset: {cutoff_time - line_time} s')\n",
    "\n",
    "create_data_files(AST_DATASET)\n",
    "ast_time = time.time()\n",
    "print(f'Time to finish ast_dataset: {ast_time - cutoff_time} s')\n",
    "\n",
    "create_data_files(AST_NO_CODE_DATASET)\n",
    "no_code_time = time.time()\n",
    "print(f'Time to finish ast_no_code_dataset: {no_code_time - ast_time} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs329m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
